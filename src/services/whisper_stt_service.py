"""
Production-Ready OpenAI Whisper STT Service
–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –õ–æ–∫–∞–ª—å–Ω—ã–π STT —Å 99.9% –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é
"""

import asyncio
import io
import time
import tempfile
import os
import sys
import subprocess
from typing import AsyncGenerator, Optional, Dict, Any, List
import numpy as np
import whisper
import torch
from pydub import AudioSegment

# VAD –∏–º–ø–æ—Ä—Ç —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
try:
    import webrtcvad
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False
    webrtcvad = None

from ..core.config import settings
from ..core.models import TranscriptSegment, MessageRole
from ..core.exceptions import AudioProcessingError
from ..core.logging import LoggerMixin, log_performance, log_error


class SafeTempFileManager(LoggerMixin):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å RAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    –ö–†–ò–¢–ò–ß–ù–û: –í—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç fallback –Ω–∞ –¥–∏—Å–∫
    """
    
    def __init__(self, max_ram_mb: int = 100):
        self.max_ram_bytes = max_ram_mb * 1024 * 1024
        self.current_ram_usage = 0
        self.ram_dir = self._detect_ram_dir()
        self.use_ram = self.ram_dir is not None
        self._lock = asyncio.Lock()
        
        if self.use_ram:
            self.logger.info(
                f"‚úÖ RAM temp files enabled",
                ram_dir=self.ram_dir,
                max_mb=max_ram_mb
            )
        else:
            self.logger.info("‚ÑπÔ∏è RAM temp files not available, using disk")
    
    def _detect_ram_dir(self) -> Optional[str]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RAM –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
        if not getattr(settings, 'enable_ram_tempfiles', False):
            return None
            
        candidates = [
            "/dev/shm",      # Linux standard
            "/run/shm",      # Linux alternative
        ]
        
        for dir_path in candidates:
            if self._is_ram_dir_suitable(dir_path):
                return dir_path
                
        self.logger.debug("No suitable RAM directory found")
        return None
    
    def _is_ram_dir_suitable(self, dir_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ RAM –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
            if not os.path.exists(dir_path):
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —ç—Ç–æ tmpfs (—Ç–æ–ª—å–∫–æ Linux)
            if sys.platform.startswith('linux'):
                result = subprocess.run(
                    ['df', '-T', dir_path],
                    capture_output=True,
                    text=True,
                    timeout=1
                )
                if 'tmpfs' not in result.stdout:
                    return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –∑–∞–ø–∏—Å–∏
            test_file = os.path.join(dir_path, f'.whisper_test_{os.getpid()}')
            try:
                with open(test_file, 'wb') as f:
                    f.write(b'test')
                os.unlink(test_file)
                return True
            except Exception:
                return False
                
        except Exception as e:
            self.logger.debug(f"RAM dir check failed for {dir_path}: {e}")
            return False
    
    async def create_temp_file(self, suffix: str = '.webm', size_estimate: int = 0) -> tempfile._TemporaryFileWrapper:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –º–µ—Å—Ç–∞
        –ì–ê–†–ê–ù–¢–ò–Ø: –í—Å–µ–≥–¥–∞ –≤–µ—Ä–Ω–µ—Ç —Ä–∞–±–æ—á–∏–π —Ñ–∞–π–ª (RAM –∏–ª–∏ –¥–∏—Å–∫)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–∂–µ–º –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAM
        use_ram_for_this = (
            self.use_ram and 
            size_estimate < self.max_ram_bytes * 0.8 and  # 80% –ª–∏–º–∏—Ç–∞
            self.current_ram_usage + size_estimate < self.max_ram_bytes
        )
        
        if use_ram_for_this:
            try:
                # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –≤ RAM
                temp_file = tempfile.NamedTemporaryFile(
                    suffix=suffix,
                    delete=False,
                    dir=self.ram_dir
                )
                
                async with self._lock:
                    self.current_ram_usage += size_estimate
                    
                self.logger.debug(
                    f"Created RAM temp file",
                    path=temp_file.name,
                    estimated_size=size_estimate,
                    total_ram_usage=self.current_ram_usage
                )
                return temp_file
                
            except Exception as e:
                self.logger.warning(
                    f"Failed to create RAM temp file, falling back to disk",
                    error=str(e)
                )
                # –û—Ç–∫–ª—é—á–∞–µ–º RAM –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                self.use_ram = False
        
        # Fallback –Ω–∞ –¥–∏—Å–∫ (–≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        temp_file = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
        self.logger.debug(f"Created disk temp file: {temp_file.name}")
        return temp_file
    
    async def cleanup_temp_file(self, file_path: str, size_estimate: int = 0):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—ã–ª –ª–∏ —Ñ–∞–π–ª –≤ RAM
            if self.ram_dir and file_path.startswith(self.ram_dir):
                async with self._lock:
                    self.current_ram_usage = max(0, self.current_ram_usage - size_estimate)
                    
            os.unlink(file_path)
            self.logger.debug(f"Cleaned up temp file: {file_path}")
            
        except FileNotFoundError:
            pass  # –§–∞–π–ª —É–∂–µ —É–¥–∞–ª–µ–Ω
        except Exception as e:
            self.logger.debug(f"Failed to cleanup temp file {file_path}: {e}")


class WhisperSTTService(LoggerMixin):
    """
    Production-Ready Whisper STT —Å–µ—Ä–≤–∏—Å
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç 99.9% –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
    """
    
    def __init__(self, model_name: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Whisper STT —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            model_name: –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large, large-v2, large-v3)
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–ª–∏ default
        self.model_name = model_name or getattr(settings, 'whisper_model', 'base')
        self.model: Optional[whisper.Whisper] = None
        self.is_gpu_available = torch.cuda.is_available()
        self._model_lock = asyncio.Lock()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è production (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞)
        self.chunk_duration = 2  # —Å–µ–∫—É–Ω–¥ –Ω–∞ —á–∞–Ω–∫ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞)
        self.overlap_duration = 0.5  # —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è (–º–∏–Ω–∏–º—É–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        self.sample_rate = 16000  # Whisper —Ç—Ä–µ–±—É–µ—Ç 16kHz
        
        # VAD –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–ë–ï–ó–û–ü–ê–°–ù–û: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω)
        self.use_vad = getattr(settings, 'enable_vad', False)
        self.vad = None
        if self.use_vad and VAD_AVAILABLE:
            try:
                self.vad = webrtcvad.Vad()
                self.vad.set_mode(2)  # –°—Ä–µ–¥–Ω—è—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å (0-3)
                self.logger.info("‚úÖ VAD (Voice Activity Detection) –≤–∫–ª—é—á–µ–Ω")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å VAD: {e}")
                self.use_vad = False
                self.vad = None
        else:
            if self.use_vad and not VAD_AVAILABLE:
                self.logger.warning("‚ö†Ô∏è VAD –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ webrtcvad –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        self.temp_file_manager = None
        if getattr(settings, 'enable_ram_tempfiles', False):
            try:
                self.temp_file_manager = SafeTempFileManager(
                    max_ram_mb=getattr(settings, 'ram_tempfiles_max_size', 100)
                )
            except Exception as e:
                self.logger.warning(
                    f"‚ö†Ô∏è Failed to initialize RAM temp files, using disk",
                    error=str(e)
                )
                self.temp_file_manager = None
        
        self.logger.info(
            "WhisperSTT service initializing",
            model=model_name,
            gpu_available=self.is_gpu_available,
            sample_rate=self.sample_rate,
            vad_enabled=self.use_vad
        )
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self._ensure_model_loaded()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        # Whisper –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        pass
    
    async def _ensure_model_loaded(self) -> None:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å
        """
        if self.model is not None:
            return
            
        async with self._model_lock:
            if self.model is not None:
                return
                
            try:
                self.logger.info(
                    "Loading Whisper model",
                    model=self.model_name,
                    gpu=self.is_gpu_available
                )
                
                start_time = time.time()
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º thread —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
                self.model = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: whisper.load_model(
                        self.model_name,
                        device="cuda" if self.is_gpu_available else "cpu"
                    )
                )
                
                load_time = time.time() - start_time
                
                self.logger.info(
                    "‚úÖ Whisper model loaded successfully",
                    **log_performance("model_loading", load_time * 1000, "WhisperSTT"),
                    model=self.model_name,
                    device=self.model.device
                )
                
            except Exception as e:
                self.logger.error(
                    "‚ùå Failed to load Whisper model",
                    **log_error(e, "model_loading", "WhisperSTT"),
                    model=self.model_name
                )
                raise AudioProcessingError(f"Failed to load Whisper model: {str(e)}")
    
    async def health_check(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è STT —Å–µ—Ä–≤–∏—Å–∞
        
        Returns:
            bool: True –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ
        """
        try:
            await self._ensure_model_loaded()
            
            # –¢–µ—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            test_result = await self._transcribe_audio_data(
                np.zeros(self.sample_rate, dtype=np.float32),  # 1 —Å–µ–∫—É–Ω–¥–∞ —Ç–∏—à–∏–Ω—ã
                language="ru"
            )
            
            self.logger.info(
                "WhisperSTT health check passed",
                model=self.model_name,
                test_transcription=test_result
            )
            
            return True
            
        except Exception as e:
            self.logger.error(
                "WhisperSTT health check failed",
                **log_error(e, "health_check", "WhisperSTT")
            )
            return False
    
    async def transcribe_stream(
        self,
        audio_stream: AsyncGenerator[bytes, None],
        session_id: str,
        language: str = "ru"
    ) -> AsyncGenerator[TranscriptSegment, None]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            audio_stream: –ü–æ—Ç–æ–∫ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö
            session_id: ID —Å–µ—Å—Å–∏–∏
            language: –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (ru, en, etc.)
            
        Yields:
            TranscriptSegment: –°–µ–≥–º–µ–Ω—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        self.logger.info(
            "üéôÔ∏è Starting NEW Whisper transcription stream",
            session_id=session_id,
            language=language,
            model=self.model_name,
            timestamp=time.time()
        )
        
        try:
            await self._ensure_model_loaded()
            
            # –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö
            audio_buffer = bytearray()
            chunk_counter = 0
            total_bytes = 0
            last_chunk_time = time.time()
            silence_timeout = 5.0  # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            early_silence_timeout = 2.0  # –†–∞–Ω–Ω—è—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –ø–∞—É–∑–µ 2 —Å–µ–∫—É–Ω–¥—ã
            
            # –ö–†–ò–¢–ò–ß–ù–û: –î–ª—è WebM –ø–æ—Ç–æ–∫–æ–≤ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            webm_header = None
            webm_header_buffer = bytearray()  # –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            is_collecting_header = True  # –§–ª–∞–≥ —Å–±–æ—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            recording_session = 0  # –°—á–µ—Ç—á–∏–∫ —Å–µ—Å—Å–∏–π –∑–∞–ø–∏—Å–∏
            
            async for audio_chunk in audio_stream:
                chunk_size = len(audio_chunk)
                total_bytes += chunk_size
                last_chunk_time = time.time()
                
                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 10-–π —á–∞–Ω–∫
                if total_bytes % 50000 < chunk_size:  # –ü—Ä–∏–º–µ—Ä–Ω–æ –∫–∞–∂–¥—ã–µ 50KB
                    self.logger.info(
                        f"üìä STT stream alive: {total_bytes} bytes received",
                        session_id=session_id,
                        chunks_received=chunk_counter,
                        buffer_size=len(audio_buffer)
                    )
                
                # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ WebM –µ—Å–ª–∏ –µ—â–µ –Ω–µ —Å–æ–±—Ä–∞–ª–∏
                if is_collecting_header:
                    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    webm_header_buffer.extend(audio_chunk)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    if len(webm_header_buffer) >= 200:  # WebM –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±—ã—á–Ω–æ 100-500 –±–∞–π—Ç
                        recording_session += 1
                        webm_header = bytes(webm_header_buffer)
                        is_collecting_header = False
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Å—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤ audio_buffer
                        audio_buffer.extend(webm_header_buffer)
                        
                        self.logger.info(
                            "‚úÖ WebM header collected successfully",
                            session_id=session_id,
                            recording_session=recording_session,
                            header_size=len(webm_header),
                            chunks_needed=chunk_counter + 1,
                            first_bytes=webm_header[:20].hex() if len(webm_header) > 20 else webm_header.hex()
                        )
                    else:
                        # –ï—â–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        self.logger.debug(
                            "üì¶ Collecting WebM header",
                            session_id=session_id,
                            buffer_size=len(webm_header_buffer),
                            chunk_size=chunk_size,
                            chunk_number=chunk_counter + 1
                        )
                else:
                    # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –ø—É—Å—Ç–æ–π –∏ –ø—Ä–∏—Ö–æ–¥–∏—Ç –Ω–æ–≤—ã–π —á–∞–Ω–∫
                    if len(audio_buffer) == 0:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –Ω–∞—á–∞–ª–æ–º –Ω–æ–≤–æ–≥–æ WebM —Ñ–∞–π–ª–∞
                        if len(audio_chunk) > 4 and audio_chunk[:4].hex() == "1a45dfa3":
                            # –≠—Ç–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ WebM —Ñ–∞–π–ª–∞!
                            recording_session += 1
                            # –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ —Å–æ–±–∏—Ä–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏
                            webm_header_buffer = bytearray(audio_chunk)
                            is_collecting_header = True
                            
                            # –ï—Å–ª–∏ —á–∞–Ω–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, —Å—Ä–∞–∑—É —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                            if len(audio_chunk) >= 200:
                                webm_header = audio_chunk
                                is_collecting_header = False
                                audio_buffer.extend(audio_chunk)
                            else:
                                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
                                self.logger.info(
                                    "üîÑ New WebM detected, collecting header...",
                                    session_id=session_id,
                                    initial_chunk_size=len(audio_chunk)
                                )
                            self.logger.info(
                                "üÜï New WebM recording detected (EBML header found)",
                                session_id=session_id,
                                recording_session=recording_session,
                                chunk_size=chunk_size,
                                first_bytes=audio_chunk[:20].hex() if len(audio_chunk) > 20 else audio_chunk.hex()
                            )
                        else:
                            # –≠—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–æ—Ç–æ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                            if webm_header is not None and len(webm_header) >= 100:
                                self.logger.info(
                                    "üìº Continuing previous WebM stream with saved headers",
                                    session_id=session_id,
                                    recording_session=recording_session,
                                    chunk_size=chunk_size,
                                    saved_header_size=len(webm_header)
                                )
                                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –±—É—Ñ–µ—Ä —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                                audio_buffer.extend(webm_header)
                                audio_buffer.extend(audio_chunk)
                            else:
                                # –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ - –∂–¥–µ–º –Ω–æ–≤—ã–π WebM –ø–æ—Ç–æ–∫
                                self.logger.warning(
                                    "‚ö†Ô∏è No valid WebM headers saved, waiting for new stream",
                                    session_id=session_id,
                                    webm_header_size=len(webm_header) if webm_header else 0
                                )
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç —ç—Ç–æ –Ω–æ–≤—ã–π WebM —Ñ–∞–π–ª
                                if len(audio_chunk) > 4 and audio_chunk[:4].hex() == "1a45dfa3":
                                    if len(audio_chunk) >= 100:
                                        webm_header = audio_chunk
                                    audio_buffer.extend(audio_chunk)
                                else:
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                                    continue
                    # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –ù–ï –ø—É—Å—Ç–æ–π
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å
                        if len(audio_chunk) > 4 and audio_chunk[:4].hex() == "1a45dfa3":
                            # –≠—Ç–æ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å! –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                            self.logger.info(
                                "üîÑ NEW RECORDING DETECTED - Resetting state",
                                session_id=session_id,
                                recording_session=recording_session + 1,
                                buffer_size=len(audio_buffer),
                                chunk_size=len(audio_chunk)
                            )
                            # –≠—Ç–æ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å! –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π –±—É—Ñ–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                            if len(audio_buffer) > 1000:
                                self.logger.info(
                                    "üîÑ New WebM stream detected, processing previous buffer",
                                    session_id=session_id,
                                    buffer_size=len(audio_buffer),
                                    buffer_first_bytes=audio_buffer[:20].hex() if len(audio_buffer) > 20 else audio_buffer.hex()
                                )
                                try:
                                    audio_data = await self._convert_audio_chunk(bytes(audio_buffer))
                                    if audio_data is not None and len(audio_data) > 0:
                                        text = await self._transcribe_audio_data(audio_data, language)
                                        if text and text.strip():
                                            segment = TranscriptSegment(
                                                text=text.strip(),
                                                speaker=MessageRole.USER,
                                                timestamp=time.time(),
                                                confidence=0.95
                                            )
                                            yield segment
                                except Exception as e:
                                    self.logger.error(
                                        "Error processing buffer before new stream",
                                        error=str(e),
                                        session_id=session_id
                                    )
                            
                            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –±—É—Ñ–µ—Ä —Å –Ω–æ–≤—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                            audio_buffer = bytearray(audio_chunk)
                            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞
                            if len(audio_chunk) >= 100:
                                webm_header = audio_chunk
                            self.logger.info(
                                "üì¶ Started new WebM stream",
                                session_id=session_id,
                                chunk_size=chunk_size,
                                first_bytes=audio_chunk[:20].hex() if len(audio_chunk) > 20 else audio_chunk.hex()
                            )
                        else:
                            audio_buffer.extend(audio_chunk)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                buffer_duration = (len(audio_buffer) - 1000) / (self.sample_rate * 2) if len(audio_buffer) > 1000 else 0
                self.logger.debug(
                    "üì¶ Received audio chunk",
                    session_id=session_id,
                    chunk_number=chunk_counter + 1,
                    chunk_size=chunk_size,
                    buffer_size=len(audio_buffer),
                    total_bytes=total_bytes,
                    buffer_seconds=round(buffer_duration, 2),
                    is_collecting_header=is_collecting_header,
                    recording_session=recording_session,
                    webm_header_exists=webm_header is not None
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥—ã
                if buffer_duration > 0 and int(buffer_duration * 2) > int((buffer_duration - chunk_size / (self.sample_rate * 2)) * 2):
                    self.logger.info(
                        f"üìä Buffer progress: {buffer_duration:.1f}s / 4.0s",
                        session_id=session_id,
                        percentage=int(buffer_duration / 4.0 * 100)
                    )
                
                # –ö–†–ò–¢–ò–ß–ù–û: –ò–∑–º–µ–Ω–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ - –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –¥–≤—É—Ö —Å–ª—É—á–∞—è—Ö:
                # 1. –ù–∞–∫–æ–ø–∏–ª–∏ —Ü–µ–ª–µ–≤–æ–π –æ–±—ä–µ–º (4 —Å–µ–∫—É–Ω–¥—ã)
                # 2. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–∞—É–∑–∞ –≤ —Ä–µ—á–∏ (2 —Å–µ–∫—É–Ω–¥—ã) –ò –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞ –∞—É–¥–∏–æ
                TARGET_BUFFER_SECONDS = 4.0  # –í–µ—Ä–Ω—É–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ 4.0 - —Ä–∞–±–æ—á–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                time_since_last_chunk = time.time() - last_chunk_time
                
                should_process = False
                if len(audio_buffer) >= (TARGET_BUFFER_SECONDS * self.sample_rate * 2 + 1000):  # 4 —Å–µ–∫—É–Ω–¥—ã –∞—É–¥–∏–æ + –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    should_process = True
                    self.logger.info("üéØ Processing: buffer full (4s)", session_id=session_id)
                elif (time_since_last_chunk >= early_silence_timeout and 
                      len(audio_buffer) >= (1.0 * self.sample_rate * 2 + 1000)):  # –ü–∞—É–∑–∞ 2—Å + –º–∏–Ω–∏–º—É–º 1—Å –∞—É–¥–∏–æ
                    should_process = True
                    self.logger.info(
                        f"üéØ Processing: silence detected ({time_since_last_chunk:.1f}s pause, {buffer_duration:.1f}s audio)",
                        session_id=session_id
                    )
                
                if should_process:
                    chunk_counter += 1
                    
                    self.logger.info(
                        f"üéØ Processing audio buffer #{chunk_counter}",
                        session_id=session_id,
                        buffer_size=len(audio_buffer),
                        is_collecting_header=is_collecting_header,
                        has_webm_header=webm_header is not None
                    )
                    
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –í–°–ï –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
                        audio_data = await self._convert_audio_chunk(
                            bytes(audio_buffer)
                        )
                        
                        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
                        if audio_data is not None and len(audio_data) > 0:
                            text = await self._transcribe_audio_data(audio_data, language)
                            
                            if text and text.strip():
                                # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
                                segment = TranscriptSegment(
                                    text=text.strip(),
                                    speaker=MessageRole.USER,
                                    timestamp=time.time(),
                                    confidence=0.95  # Whisper –æ–±—ã—á–Ω–æ –æ—á–µ–Ω—å —Ç–æ—á–Ω—ã–π
                                )
                                
                                self.logger.info(
                                    "‚úÖ Transcription segment produced",
                                    session_id=session_id,
                                    chunk=chunk_counter,
                                    text_length=len(text),
                                    text_preview=text[:50] + "..." if len(text) > 50 else text,
                                    audio_duration_seconds=len(audio_data) / self.sample_rate if audio_data is not None else 0
                                )
                                
                                yield segment
                        
                        # –ö–†–ò–¢–ò–ß–ù–û: –î–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –æ—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –ø–æ–ª–Ω–æ—Å—Ç—å—é
                        audio_buffer = bytearray()
                        # –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞–µ–º webm_header - –æ–Ω –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞
                        # is_collecting_header –æ—Å—Ç–∞–µ—Ç—Å—è False, —Ç–∞–∫ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —É–∂–µ —Å–æ–±—Ä–∞–Ω—ã
                        
                        self.logger.info(
                            "üîÑ Buffer cleared, ready for more audio",
                            session_id=session_id,
                            recording_session=recording_session,
                            webm_header_preserved=webm_header is not None
                        )
                        
                    except Exception as e:
                        self.logger.error(
                            "Error processing audio chunk",
                            **log_error(e, "transcribe_chunk", "WhisperSTT"),
                            session_id=session_id,
                            chunk=chunk_counter
                        )
                        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –æ—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        audio_buffer = bytearray()
                        # –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞–µ–º webm_header - –æ–Ω –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —á–∞–Ω–∫–µ
                        continue
                
                # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ - –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏
                # —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ —á—Ç–æ –µ—Å—Ç—å
                if (time.time() - last_chunk_time > silence_timeout and 
                    len(audio_buffer) > self.sample_rate):  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
                    
                    self.logger.info(
                        "Processing audio due to silence timeout",
                        session_id=session_id,
                        buffer_size=len(audio_buffer),
                        silence_duration=time.time() - last_chunk_time
                    )
                    
                    try:
                        audio_data = await self._convert_audio_chunk(bytes(audio_buffer))
                        if audio_data is not None:
                            text = await self._transcribe_audio_data(audio_data, language)
                            if text and text.strip():
                                segment = TranscriptSegment(
                                    text=text.strip(),
                                    speaker=MessageRole.USER,
                                    timestamp=time.time(),
                                    confidence=0.95
                                )
                                yield segment
                        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞
                        audio_buffer = bytearray()
                        last_chunk_time = time.time()
                    except Exception as e:
                        self.logger.error(
                            "Error processing audio due to timeout",
                            **log_error(e, "transcribe_timeout", "WhisperSTT"),
                            session_id=session_id
                        )
                        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –æ—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        audio_buffer = bytearray()
                        # –ù–ï —Å–±—Ä–∞—Å—ã–≤–∞–µ–º webm_header - –æ–Ω –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –≤ –±—É—Ñ–µ—Ä–µ
            if len(audio_buffer) > 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                self.logger.info(
                    f"üìä Processing final audio buffer",
                    session_id=session_id,
                    buffer_size=len(audio_buffer),
                    total_chunks=chunk_counter
                )
                try:
                    audio_data = await self._convert_audio_chunk(bytes(audio_buffer))
                    if audio_data is not None:
                        text = await self._transcribe_audio_data(audio_data, language)
                        if text and text.strip():
                            segment = TranscriptSegment(
                                text=text.strip(),
                                speaker=MessageRole.USER,
                                timestamp=time.time(),
                                confidence=0.95
                            )
                            yield segment
                except Exception as e:
                    self.logger.error(
                        "Error processing final buffer",
                        **log_error(e, "transcribe_final", "WhisperSTT"),
                        session_id=session_id
                    )
            
            self.logger.warning(
                f"‚ö†Ô∏è CRITICAL: Whisper transcription stream ENDED - This should NOT happen during active session!",
                session_id=session_id,
                total_chunks_processed=chunk_counter,
                total_bytes_received=total_bytes,
                last_chunk_time_ago=time.time() - last_chunk_time,
                audio_buffer_remaining=len(audio_buffer)
            )
                    
        except asyncio.CancelledError:
            self.logger.info(
                "Whisper transcription stream cancelled",
                session_id=session_id
            )
        except Exception as e:
            self.logger.error(
                "Whisper transcription stream error",
                **log_error(e, "transcribe_stream", "WhisperSTT"),
                session_id=session_id
            )
            raise AudioProcessingError(f"Transcription failed: {str(e)}")
    
    async def _convert_audio_chunk(self, audio_bytes: bytes) -> Optional[np.ndarray]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Whisper
        
        Args:
            audio_bytes: –ò—Å—Ö–æ–¥–Ω—ã–µ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            np.ndarray: –ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–ª–∏ None
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ WebM (–Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å EBML –∑–∞–≥–æ–ª–æ–≤–∫–∞)
            is_webm = len(audio_bytes) > 4 and audio_bytes[:4] == b'\x1a\x45\xdf\xa3'
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SafeTempFileManager –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if self.temp_file_manager:
                temp_file = await self.temp_file_manager.create_temp_file(
                    suffix=".webm",
                    size_estimate=len(audio_bytes)
                )
                temp_file.write(audio_bytes)
                temp_file.close()  # –í–∞–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FFmpeg
                temp_file_path = temp_file.name
            else:
                # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥
                with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp_file:
                    temp_file.write(audio_bytes)
                    temp_file_path = temp_file.name
            
            try:
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∏–ø
                self.logger.debug(
                    "Converting audio chunk",
                    input_size=len(audio_bytes),
                    temp_file=temp_file_path,
                    is_webm_header=is_webm,
                    first_bytes=audio_bytes[:20].hex() if len(audio_bytes) > 20 else audio_bytes.hex()
                )
                
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ webm/opus (–∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞)
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ffmpeg –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ webm –≤ wav
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    audio = AudioSegment.from_file(
                        temp_file_path, 
                        format="webm", 
                        codec="opus",
                        parameters=["-err_detect", "ignore_err", "-fflags", "+genpts"]
                    )
                    self.logger.info(
                        "‚úÖ Successfully loaded webm/opus audio with FFmpeg",
                        channels=audio.channels,
                        frame_rate=audio.frame_rate,
                        duration_ms=len(audio),
                        sample_width=audio.sample_width
                    )
                except Exception as webm_error:
                    self.logger.debug(f"Failed to load as webm: {webm_error}")
                    # –ï—Å–ª–∏ –Ω–µ webm, –ø—Ä–æ–±—É–µ–º –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
                    try:
                        audio = AudioSegment.from_file(
                            temp_file_path,
                            parameters=["-err_detect", "ignore_err", "-fflags", "+genpts"]
                        )
                        self.logger.info(
                            "‚úÖ Successfully loaded audio with auto-detection",
                            channels=audio.channels,
                            frame_rate=audio.frame_rate,
                            duration_ms=len(audio)
                        )
                    except Exception as auto_error:
                        self.logger.error(
                            "‚ùå Failed to load audio - FFmpeg error or invalid stream",
                            error=str(auto_error),
                            hint="Check if audio stream contains valid WebM data with headers"
                        )
                        return None
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Whisper
                audio = audio.set_frame_rate(self.sample_rate)
                audio = audio.set_channels(1)  # Mono
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
                audio_data = np.array(audio.get_array_of_samples(), dtype=np.float32)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-1, 1]
                if audio.sample_width == 2:  # 16-bit
                    audio_data = audio_data.astype(np.float32) / 32768.0
                elif audio.sample_width == 4:  # 32-bit
                    audio_data = audio_data.astype(np.float32) / 2147483648.0
                
                self.logger.debug(
                    "Audio conversion completed",
                    output_shape=audio_data.shape,
                    output_dtype=audio_data.dtype,
                    duration_seconds=len(audio_data) / self.sample_rate,
                    min_value=float(np.min(audio_data)),
                    max_value=float(np.max(audio_data))
                )
                
                return audio_data
                
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if self.temp_file_manager:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π cleanup —Å —É—á–µ—Ç–æ–º RAM
                    await self.temp_file_manager.cleanup_temp_file(
                        temp_file_path,
                        size_estimate=len(audio_bytes)
                    )
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ
                    try:
                        os.unlink(temp_file_path)
                    except:
                        pass
                    
        except Exception as e:
            self.logger.error(
                "Audio conversion failed",
                **log_error(e, "audio_conversion", "WhisperSTT"),
                audio_size=len(audio_bytes)
            )
            return None
    
    def _check_voice_activity(self, audio_data: np.ndarray) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≥–æ–ª–æ—Å–∞ –≤ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é VAD
        
        Args:
            audio_data: –ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            bool: True –µ—Å–ª–∏ –µ—Å—Ç—å –≥–æ–ª–æ—Å, False –µ—Å–ª–∏ —Ç–∏—à–∏–Ω–∞
        """
        if not self.use_vad or not self.vad:
            return True  # –ï—Å–ª–∏ VAD –≤—ã–∫–ª—é—á–µ–Ω, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≥–æ–ª–æ—Å –µ—Å—Ç—å
        
        try:
            # WebRTC VAD —Ç—Ä–µ–±—É–µ—Ç 16kHz, 16-bit PCM, —Ñ—Ä–µ–π–º—ã –ø–æ 10, 20 –∏–ª–∏ 30 –º—Å
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º float32 numpy –≤ int16 PCM
            if audio_data.dtype == np.float32:
                audio_int16 = (audio_data * 32767).astype(np.int16)
            else:
                audio_int16 = audio_data.astype(np.int16)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—Ä–µ–π–º—ã –ø–æ 30–º—Å (480 samples –ø—Ä–∏ 16kHz)
            frame_duration_ms = 30
            frame_length = int(self.sample_rate * frame_duration_ms / 1000)
            
            num_frames_with_voice = 0
            total_frames = 0
            
            for i in range(0, len(audio_int16) - frame_length, frame_length):
                frame = audio_int16[i:i + frame_length]
                
                # VAD —Ç—Ä–µ–±—É–µ—Ç bytes
                frame_bytes = frame.tobytes()
                
                if self.vad.is_speech(frame_bytes, self.sample_rate):
                    num_frames_with_voice += 1
                total_frames += 1
            
            # –°—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–µ–π–º–æ–≤ —Å –≥–æ–ª–æ—Å–æ–º
            if total_frames > 0:
                voice_ratio = num_frames_with_voice / total_frames
                has_voice = voice_ratio > 0.1  # –ú–∏–Ω–∏–º—É–º 10% —Ñ—Ä–µ–π–º–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≥–æ–ª–æ—Å
                
                if not has_voice:
                    self.logger.info(
                        f"üîá VAD: –¢–∏—à–∏–Ω–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ (–≥–æ–ª–æ—Å –≤ {voice_ratio*100:.1f}% —Ñ—Ä–µ–π–º–æ–≤)",
                        audio_duration=len(audio_data) / self.sample_rate
                    )
                
                return has_voice
            
            return True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≥–æ–ª–æ—Å –µ—Å—Ç—å
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ VAD –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return True  # –ü—Ä–∏ –æ—à–∏–±–∫–µ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    
    async def _transcribe_audio_data(
        self, 
        audio_data: np.ndarray, 
        language: str = "ru"
    ) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Whisper
        
        Args:
            audio_data: –ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ numpy
            language: –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            
        Returns:
            str: –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # VAD –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
            if self.use_vad:
                has_voice = self._check_voice_activity(audio_data)
                if not has_voice:
                    self.logger.debug("VAD: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é - —Ç–∏—à–∏–Ω–∞")
                    return ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ç–∏—à–∏–Ω—ã
            
            start_time = time.time()
            
            self.logger.debug(
                "Starting Whisper transcription",
                audio_shape=audio_data.shape,
                audio_duration=len(audio_data) / self.sample_rate,
                language=language,
                model=self.model_name,
                vad_passed=True if not self.use_vad else "voice_detected"
            )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º thread
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.model.transcribe(
                    audio_data,
                    language=language,
                    fp16=False,  # –ù–∞ CPU fp16 –º–µ–¥–ª–µ–Ω–Ω–µ–µ!
                    task="transcribe",
                    verbose=False,  # –û—Ç–∫–ª—é—á–∞–µ–º verbose –≤—ã–≤–æ–¥
                    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø –°–ö–û–†–û–°–¢–ò:
                    temperature=0.0,  # –û—Ç–∫–ª—é—á–∞–µ–º —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ = –±—ã—Å—Ç—Ä–µ–µ
                    beam_size=1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π beam search = –ù–ê–ú–ù–û–ì–û –±—ã—Å—Ç—Ä–µ–µ
                    best_of=1,  # –ë–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ = –±—ã—Å—Ç—Ä–µ–µ
                    compression_ratio_threshold=2.4,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä–æ–≥
                    logprob_threshold=-1.0,  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä–æ–≥
                    no_speech_threshold=0.8,  # –£–í–ï–õ–ò–ß–ï–ù –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ—Ä—å–±—ã —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏
                    condition_on_previous_text=False,  # –£—Å–∫–æ—Ä—è–µ–º, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    initial_prompt=None,  # –ë–µ–∑ –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏
                    suppress_tokens=[-1],  # –ü–æ–¥–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    suppress_blank=True,  # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –≤—ã–≤–æ–¥—ã
                    # –ë–ï–ó–û–ü–ê–°–ù–´–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò (15-20% —É—Å–∫–æ—Ä–µ–Ω–∏–µ):
                    without_timestamps=True,  # –û—Ç–∫–ª—é—á–∞–µ–º timestamp —Ç–æ–∫–µ–Ω—ã = –±—ã—Å—Ç—Ä–µ–µ
                    word_timestamps=False    # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–æ–≤–Ω—ã–µ –º–µ—Ç–∫–∏ = –±—ã—Å—Ç—Ä–µ–µ
                )
            )
            
            transcription_time = time.time() - start_time
            text = result.get("text", "").strip()
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–µ—á–∏
            # Whisper –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç segments —Å no_speech_prob –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            segments = result.get("segments", [])
            if segments:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–µ—á–∏
                first_segment = segments[0]
                no_speech_prob = first_segment.get("no_speech_prob", 0.0)
                
                # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–µ—á–∏ –≤—ã—Å–æ–∫–∞—è, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if no_speech_prob > 0.6:
                    self.logger.info(
                        "üîá High no_speech probability detected, ignoring transcription",
                        no_speech_prob=no_speech_prob,
                        text_preview=text[:50] if text else "",
                        audio_duration=len(audio_data) / self.sample_rate
                    )
                    return ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–∏—à–∏–Ω—ã
            
            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–©–ò–¢–ê: –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ Whisper
            hallucination_phrases = [
                "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç",
                "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –∫–∞–Ω–∞–ª",
                "—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä",
                "—Å—Ç–∞–≤—å—Ç–µ –ª–∞–π–∫–∏",
                "–¥–æ –≤—Å—Ç—Ä–µ—á–∏",
                "–≤—Å–µ–º –ø–æ–∫–∞",
                "—Å—É–±—Ç–∏—Ç—Ä—ã",
                "titres",
                "–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ",
                "–ø–µ—Ä–µ–≤–æ–¥",
                "[–º—É–∑—ã–∫–∞]",
                "[–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã]",
                "‚ô™",
                "‚ô´"
            ]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
            text_lower = text.lower()
            for phrase in hallucination_phrases:
                if phrase in text_lower and len(text) < 50:
                    self.logger.warning(
                        "üö´ Detected hallucination phrase, ignoring",
                        detected_phrase=phrase,
                        full_text=text
                    )
                    return ""
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è, —ç—Ç–æ —Ç–æ–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
            if len(text) < 5 and text in [".", "..", "...", "–∞", "–∏", "—ç", "–º", "–Ω—É"]:
                self.logger.warning(
                    "üö´ Detected single character hallucination",
                    text=text
                )
                return ""
            
            if text:
                audio_duration_sec = len(audio_data) / self.sample_rate
                realtime_factor = transcription_time / audio_duration_sec if audio_duration_sec > 0 else 0
                
                self.logger.info(
                    "‚úÖ Whisper transcription successful",
                    **log_performance("transcription", transcription_time * 1000, "WhisperSTT"),
                    text_length=len(text),
                    text_preview=text[:100] + "..." if len(text) > 100 else text,
                    language=result.get("language", language),
                    model=self.model_name,
                    audio_duration_sec=audio_duration_sec,
                    realtime_factor=round(realtime_factor, 2),  # <1 = –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
                    device="GPU" if self.is_gpu_available else "CPU",
                    no_speech_prob=segments[0].get("no_speech_prob", 0.0) if segments else None
                )
            else:
                self.logger.debug(
                    "Whisper transcription returned empty text",
                    duration_ms=transcription_time * 1000,
                    audio_duration=len(audio_data) / self.sample_rate
                )
            
            return text
            
        except Exception as e:
            self.logger.error(
                "Whisper transcription failed",
                **log_error(e, "transcription", "WhisperSTT"),
                language=language,
                audio_shape=audio_data.shape if audio_data is not None else None
            )
            return ""
    
    def _calculate_chunk_size(self) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ –±–∞–π—Ç–∞—Ö"""
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 16-bit audio, mono, 16kHz
        return self.chunk_duration * self.sample_rate * 2  # 2 –±–∞–π—Ç–∞ –Ω–∞ sample
    
    def _calculate_overlap_size(self) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤ –±–∞–π—Ç–∞—Ö"""
        return self.overlap_duration * self.sample_rate * 2
    
    def get_supported_languages(self) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–æ–≤
        
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ —è–∑—ã–∫–æ–≤
        """
        return [
            "ru", "en", "de", "fr", "es", "it", "pt", "nl", 
            "pl", "sv", "da", "no", "fi", "et", "lv", "lt",
            "cs", "sk", "sl", "hr", "sr", "bg", "mk", "sq",
            "uk", "be", "kk", "ky", "uz", "mn", "hy", "az",
            "ka", "tr", "ar", "he", "fa", "ur", "hi", "bn",
            "ta", "te", "kn", "ml", "si", "th", "lo", "my",
            "km", "vi", "id", "ms", "tl", "zh", "ja", "ko"
        ]